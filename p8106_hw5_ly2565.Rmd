---
title: "P8106 HW5"
author: "Lin Yang"
output: github_document
--- 

  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(tidyverse)
library(caret)
library(e1071)
library(kernlab)
```

## Problem 11

```{r}
auto <- read.csv("data/auto.csv") %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  mutate(origin = as.factor(origin),
         mpg_cat = factor(mpg_cat, levels = c("low", "high")))
str(auto)

set.seed(2022)
trainRows <- createDataPartition(y = auto$mpg_cat, p = 0.7, list = FALSE)
auto_train <- auto[trainRows, ]
auto_test <- auto[-trainRows, ]
```

### (a) Support Vector Classifier (linear kernel)

```{r, dpi=300}
set.seed(2022)
linear.tune <- tune.svm(mpg_cat ~ . , 
                        data = auto_train, 
                        kernel = "linear", 
                        cost = exp(seq(-6, -1, len = 50)),
                        scale = TRUE)
plot(linear.tune)
best.linear <- linear.tune$best.model
summary(best.linear)
```

Through 10-fold CV, the best tuning parameter (cost) of the support vector classifier is selected to be 0.019. There are 107 support vectors in the optimal support vector classifier with a linear kernel (53 from the low gas mileage level, 54 from the high mileage level). The training and test error rates are found to be 7.61% (1-0.9239 ) and 9.48% (1-0.9052). 

```{r}
#train error
pred.linear.train <- predict(best.linear, newdata = auto_train)
confusionMatrix(data = pred.linear.train, 
                reference = auto_train$mpg_cat,
                positive = "high")

#test error
pred.linear.test <- predict(best.linear, newdata = auto_test)
confusionMatrix(data = pred.linear.test, 
                reference = auto_test$mpg_cat,
                positive = "high")
```

### (b) Support Vector Machine (radial kernel)

```{r, dpi=300}
set.seed(2022)
radial.tune <- tune.svm(mpg_cat ~ . , 
                        data = auto_train, 
                        kernel = "radial", 
                        cost = exp(seq(-1, 5, len = 50)),
                        gamma = exp(seq(-5, -3, len = 20)))

plot(radial.tune, transform.y = log, transform.x = log, 
     color.palette = terrain.colors)

best.radial <- radial.tune$best.model
summary(best.radial)
radial.tune$best.parameters
```

Through 10-fold CV, the best tuning parameters (cost and gamma) of the support vector machine are selected to be 0.0174 and 34.14. There are 59 support vectors in the optimal support vector machine with a radial kernel (29 from the low gas mileage level, 30 from the high mileage level). The training and test error rates are found to be 6.16% (1-0.9384) and 9.48% (1-0.9052). The optimal support vector machine performs slightly better than the optimal support vector classifier in terms of the train errors. However, they have the same test error rates. 

```{r}
#train error
pred.radial.train <- predict(best.radial, newdata = auto_train)
confusionMatrix(data = pred.radial.train, 
                reference = auto_train$mpg_cat,
                positive = "high")

#test error
pred.radial.test <- predict(best.radial, newdata = auto_test)
confusionMatrix(data = pred.radial.test, 
                reference = auto_test$mpg_cat,
                positive = "high")
```


